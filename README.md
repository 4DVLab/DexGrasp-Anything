<br>
<p align="center">
<h1 align="center"><strong> DexGraspAnything:TowardsUniversalRoboticDexterousGrasping
 withPhysicsAwareness
</strong></h1>
  <p align="center">
      <strong><span style="color: red;">CVPR 2025</span></strong>
    <br>
   <a href='https://ymzhong66.github.io' target='_blank'>Yiming Zhong*</a>&emsp;
   <a href='https://github.com/Kenny-K' target='_blank'>Qi Jiang*</a>&emsp;
   <a href='https://sist.shanghaitech.edu.cn/yujingyi/main.htm' target='_blank'>Jingyi Yu</a>&emsp;
   <a href='https://yuexinma.me' target='_blank'>Yuexin Ma</a>&emsp;
    <br>
    ShanghaiTech University    
    <br>
    *Indicates Equal Contribution
    <br>
  </p>
</p>

  
<!--
<p align="center">
  <a href="https://dexgraspanything.github.io/"><b>📖 Project Page</b></a> |
  <a href="https://dexgraspanything.github.io/"><b>📄 Paper Link</b></a> |
</p>

</div>

> We introduce SeqAfford, a Multi-Modal Language Model (MLLM) capable of serialized affordance inference implied in human instructions: 1) Single Affordance Reasoning; 2) Sequential Affordance Reasoning; 3) Sequential Affordance Reasoning with Multiple Objects

<div align="center">
    <img src="fig1.png" height=500>
</div>
-->
## 📣 News
- [2/27/2025] 🎉🎉🎉DexGraspAnything has been accepted by CVPR 2025!!!🎉🎉🎉
<!--
## 😲 Results
Please refer to our [homepage](https://seq-afford.github.io) for more thrilling results!
-->

## 🛠️ Setup
- Comming Soon...


## 🚩 Plan
- [x] Paper Released.
- [ ] Source Code and Pretrained Weights.
- [ ] Dataset.
<!-- --- -->


<!--
## 🎫 License

For academic use, this project is licensed under [the 2-clause BSD License](https://opensource.org/license/bsd-2-clause). 

## 🖊️ Citation
```
@article{yu2024seqafford,
        title={SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large Language Model},
        author={Yu, Chunlin and Wang, Hanqing and Shi, Ye and Luo, Haoyang and Yang, Sibei and Yu, Jingyi and Wang, Jingya},
        journal={arXiv preprint arXiv:2412.01550},
        year={2024}
      }

```
 -->
