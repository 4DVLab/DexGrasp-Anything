<br>
<p align="center">
<h1 align="center"><strong> DexGraspAnything:TowardsUniversalRoboticDexterousGrasping
 withPhysicsAwareness
</strong></h1>
  <p align="center">
      <strong><span style="color: red;">CVPR 2025</span></strong>
    <br>
   <a href='https://ymzhong66.github.io' target='_blank'>Yiming Zhong*</a>&emsp;
   Qi Jiang*</a>&emsp;
   Jingyi Yu</a>&emsp;
   Yuexin Ma</a>&emsp;
    <br>
    ShanghaiTech University    
    <br>
    *Indicates Equal Contribution
    <br>
  </p>
</p>

  

<p align="center">
  <a><b>📖 Project Page</b></a> |
  <a><b>📄 Paper Link</b></a> |
</p>

</div>
<!--
> We introduce SeqAfford, a Multi-Modal Language Model (MLLM) capable of serialized affordance inference implied in human instructions: 1) Single Affordance Reasoning; 2) Sequential Affordance Reasoning; 3) Sequential Affordance Reasoning with Multiple Objects-->
<!--
<div align="center">
    <img src="fig1.png" height=500>
</div>
<!--
## 📣 News
- [2/27/2025] 🎉🎉🎉SeqAfford has been accepted by CVPR 2025!!!🎉🎉🎉
- [12/2/2024] SeqAfford has been released on Arxiv now!!!
<!--
## 😲 Results
Please refer to our [homepage](https://seq-afford.github.io) for more thrilling results!

-->
## 🛠️ Setup
- Comming Soon...


## 🚩 Plan
- [x] Paper Released.
- [ ] Source Code.
- [ ] Dataset.
<!-- --- -->


## 🎫 License

For academic use, this project is licensed under [the 2-clause BSD License](https://opensource.org/license/bsd-2-clause). 

## 🖊️ Citation
```
@article{yu2024seqafford,
        title={SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large Language Model},
        author={Yu, Chunlin and Wang, Hanqing and Shi, Ye and Luo, Haoyang and Yang, Sibei and Yu, Jingyi and Wang, Jingya},
        journal={arXiv preprint arXiv:2412.01550},
        year={2024}
      }

```
